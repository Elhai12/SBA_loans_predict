{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0p6SrB9fHOMgx+8qnX1Ay",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elhai12/SBA_loans_predict/blob/main/Feature_Engineering_%26_Feature_Selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ce_qR9WQUwsV",
        "outputId": "74604ad0-cac6-4719-be57-871509fba3a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SBA_loans_predict'...\n",
            "remote: Enumerating objects: 214, done.\u001b[K\n",
            "remote: Counting objects: 100% (101/101), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n"
          ]
        }
      ],
      "source": [
        "#Clone git to get the files\n",
        "!git clone https://github.com/Elhai12/SBA_loans_predict.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import re\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import time\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.express as px\n",
        "import plotly.subplots as sp\n",
        "from scipy.stats import mannwhitneyu,chisquare,chi2_contingency\n",
        "import plotly.express as px\n",
        "from SBA_loans_predict import Functions_for_EDA\n",
        "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
        "from geopy.geocoders import Nominatim\n",
        "from folium import plugins\n",
        "import folium\n",
        "from wordcloud import WordCloud\n",
        "from scipy.stats import ttest_ind\n",
        "from scipy.stats import chi2_contingency\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "a4zeGxd3PVuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_pickle('/content/SBA_loans_predict/data_files/manipulated_data.pkl')"
      ],
      "metadata": {
        "id": "A3WJN6neUzcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "8b8myH38dS3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineer"
      ],
      "metadata": {
        "id": "wQMAAXQVvIlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the Zipcode and create intercative map\n",
        "df_code = pd.read_pickle('/content/SBA_loans_predict/data_files/text_zip.pkl')\n",
        "\n",
        "# def get_coordinates_from_nominatim(postal_code):\n",
        "#     geolocator = Nominatim(user_agent=\"Nominatim\")\n",
        "#     location = geolocator.geocode({\"postalcode\": postal_code},country_codes='Florida',language=\"en\",timeout=5)\n",
        "\n",
        "#     if location:\n",
        "#       if 'Florida' in location.address:\n",
        "#         print(f\"Coordinates found for postal code: {postal_code}\")\n",
        "#         return location.longitude, location.latitude\n",
        "#       else:\n",
        "#         print(f\"Coordinates not found for postal code: {postal_code}\")\n",
        "#         return None, None\n",
        "\n",
        "#     else:\n",
        "#       print(f\"Coordinates not found for postal code: {postal_code}\")\n",
        "#       return None, None\n",
        "\n",
        "# coordinates_dict = {}\n",
        "# i = 1\n",
        "# for postal_code in df_code['Zip'].unique().tolist():\n",
        "#     print(i)\n",
        "#     Longitude, Latitude = get_coordinates_from_nominatim(postal_code)\n",
        "#     coordinates_dict[postal_code] = [Latitude, Longitude]\n",
        "#     i+=1\n",
        "# coordinates_dict"
      ],
      "metadata": {
        "id": "Rxix_CvkvIPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_coordinates = pd.DataFrame.from_dict(\n",
        "#     coordinates_dict,\n",
        "#     orient='index',\n",
        "#     columns=['Longitude','Latitude']\n",
        "# )\n",
        "# df_coordinates.reset_index(inplace=True)\n",
        "# df_coordinates.rename(columns={'index': 'Postal_Code'}, inplace=True)\n",
        "# df_coordinates.to_csv(\"codes_coordinates.csv\",index=False)\n"
      ],
      "metadata": {
        "id": "7Q3HJiNqEDcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_coordinates = pd.read_csv('/content/SBA_loans_predict/data_files/codes_coordinates.csv')\n",
        "df_coordinates"
      ],
      "metadata": {
        "id": "rVlew8E9jzgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_coordinates = df_coordinates.dropna()"
      ],
      "metadata": {
        "id": "0hptOnbyz-6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_coordinates = df_coordinates.dropna()\n",
        "df_merge = pd.merge(df_coordinates,df_code,left_on='Postal_Code',right_on='Zip',how='inner')\n",
        "#Get the loans status\n",
        "df_coordinates_merge = df_merge[['Postal_Code','Latitude','Longitude','MIS_Status']]\n",
        "#Aggregate for ceach erea how much paid and how much CHGOFF\n",
        "df_agg = df_coordinates_merge.groupby(['Postal_Code','Latitude','Longitude'])['MIS_Status'].value_counts().unstack().fillna(0).reset_index()\n",
        "df_agg.columns = ['Postal_Code','Latitude','Longitude','CHGOFF','PayFull']\n",
        "\n",
        "#Get the percent of CHGOFF, gropping for over/under 50%, add total\n",
        "df_agg['prc_CHGOFF'] = round(df_agg['CHGOFF']/(df_agg['CHGOFF']+df_agg['PayFull']),3)\n",
        "df_agg['Over_50'] = np.where(df_agg['prc_CHGOFF']>=0.5,\"Over_50_percent\",\"Under_50_percent\")\n",
        "df_agg['Total'] = df_agg['CHGOFF']+df_agg['PayFull']\n",
        "\n",
        "fig = px.scatter_mapbox(\n",
        "    df_agg,\n",
        "    lat=\"Longitude\",\n",
        "    lon=\"Latitude\",\n",
        "    hover_name=\"Postal_Code\",\n",
        "    hover_data={\n",
        "        \"Latitude\": False,\n",
        "        \"Longitude\": False,\n",
        "        \"Postal_Code\": False,\n",
        "        \"CHGOFF\": True,\n",
        "        \"PayFull\": True,\n",
        "    },\n",
        "\n",
        "    size= 'Total',\n",
        "    color= 'Over_50'\n",
        "\n",
        ")\n",
        "# Customize map layout\n",
        "fig.update_traces(marker=dict(sizeref=1, sizemin=5))\n",
        "fig.update_layout(\n",
        "    mapbox_style=\"open-street-map\",\n",
        "    mapbox_center={\"lat\": df_agg['Longitude'].mean(), \"lon\": df_agg['Latitude'].mean()},\n",
        "    mapbox_zoom=5,\n",
        "    title=\"Zip Code Map\"\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "NJQKbzJw0kjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the names of the companies and use text analsys for most word for each group\n",
        "\n",
        "df_name = df_code[['Name','MIS_Status']]\n",
        "\n",
        "group_name_pay = df_name[df_name['MIS_Status']=='P I F']['Name']\n",
        "group_name_chagoff = df_name[df_name['MIS_Status']=='CHGOFF']['Name']\n",
        "\n",
        "text_pay= \" \".join(group_name_pay).lower()\n",
        "text_chagoff= \" \".join(group_name_chagoff).lower()\n",
        "\n",
        "#Function for remove words not relevant\n",
        "def remove_special_characters(text,remove_words):\n",
        "    clean_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    words= clean_text.split()\n",
        "    filtered_words = [word for word in words if word not in remove_words and len(word)>1]\n",
        "    filtered_text = \" \".join(filtered_words)\n",
        "\n",
        "    return filtered_text\n",
        "\n",
        "remove_words = ['inc','inn','llc','corporation','florida','service','international'\n",
        ",'corp','group','usa','enterprise','enterprises','services','company','dba','center','co','associate','associates','system',\n",
        "'systems','solution','solutions','american','pa','auto','product','products','md','management']\n",
        "\n",
        "text_pay = remove_special_characters(text_pay,remove_words)\n",
        "text_chagoff = remove_special_characters(text_chagoff,remove_words)"
      ],
      "metadata": {
        "id": "tcCjLX4J07WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function create word cloud\n",
        "def gen_word_cloud(text,title):\n",
        "  wordcloud = WordCloud(width=800, height=400, background_color='white',colormap='viridis').generate(text)\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.imshow(wordcloud, interpolation='bilinear')\n",
        "  plt.axis('off')\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "\n",
        "gen_word_cloud(text_pay,'Pay Full')\n",
        "gen_word_cloud(text_chagoff,'CHGOFF')"
      ],
      "metadata": {
        "id": "VXkvaCx4AcZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tests on the target group"
      ],
      "metadata": {
        "id": "RjOO6xE7aMVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#A T-test was conducted on the approved loan amounts to compare those who fully paid those who did not.\n",
        "paid = df[df['MIS_Status'] == 1]['GrAppv']\n",
        "unpaid = df[df['MIS_Status'] == 0]['GrAppv']\n",
        "t_stat, p_value = ttest_ind(paid, unpaid)\n",
        "\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"P-value: {p_value}\")"
      ],
      "metadata": {
        "id": "9WHrcM9-ZXod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A Chi-Square test was conducted on the company sector to compare those who fully paid those who did not.\n",
        "contingency_table = pd.crosstab( df['Sector_group'],df['MIS_Status'])\n",
        "\n",
        "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "print(f\"Chi-Square Statistic: {chi2}\")\n",
        "print(f\"P-value: {round(p_value,5)}\")"
      ],
      "metadata": {
        "id": "HlJeqnIPcG0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature selction"
      ],
      "metadata": {
        "id": "v3kcR16iXaRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert category to encoding for fit models and choose features\n",
        "df_coding = df.copy()\n",
        "\n",
        "cat_cols = df_coding.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    df_coding[col] = encoder.fit_transform(df_coding[col])"
      ],
      "metadata": {
        "id": "p2ffFkh5fHro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define X and Y\n",
        "X = df_coding.drop(['MIS_Status'], axis=1)\n",
        "y = df_coding['MIS_Status']"
      ],
      "metadata": {
        "id": "wkP5hZPcfhNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit models and determine if a feature is selected (1) or not (0)\n",
        "lasso = Lasso(alpha=0.01).fit(X, y)\n",
        "lasso_selected = (np.abs(lasso.coef_) > 0).astype(int)\n",
        "\n",
        "# Fit Ridge model\n",
        "ridge = Ridge(alpha=0.01).fit(X, y)\n",
        "ridge_selected = (np.abs(ridge.coef_) > 0).astype(int)\n",
        "\n",
        "svm = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
        "svm_selected = (np.abs(svm.coef_[0]) > 0).astype(int)\n",
        "\n",
        "XGBoost = xgb.XGBClassifier().fit(X, y)\n",
        "XGBoost_selected = (XGBoost.feature_importances_ > 0).astype(int)\n",
        "\n",
        "rf = RandomForestClassifier().fit(X, y)\n",
        "rf_selected = (rf.feature_importances_ > 0).astype(int)\n",
        "\n",
        "# Create a DataFrame to store results\n",
        "selection_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Lasso': lasso_selected,\n",
        "    'SVM': svm_selected,\n",
        "    'XGBoost': XGBoost_selected,\n",
        "    'RandomForest': rf_selected,\n",
        "    'Ridge': ridge_selected\n",
        "})\n",
        "\n",
        "# Sum the number of selections for each feature\n",
        "selection_df['Sum'] = selection_df[['Lasso', 'SVM', 'XGBoost', 'RandomForest','Ridge']].sum(axis=1)\n",
        "\n",
        "# Output the results\n",
        "print(selection_df)"
      ],
      "metadata": {
        "id": "BMYAAPtsEOsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['NewExist'].value_counts()"
      ],
      "metadata": {
        "id": "MyIGeY_Ziz_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat_selction = selection_df[selection_df['Sum']>=3]['Feature'].tolist()\n",
        "print(len(feat_selction))\n",
        "\n",
        "print(feat_selction)"
      ],
      "metadata": {
        "id": "9dtUj3utg17Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Label Encoding and One-Hot"
      ],
      "metadata": {
        "id": "7qkoq2SJdNxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = df[feat_selction].select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "cols_labels = ['Sector_group','Category_bank','Region','Bank_State_group']\n",
        "cols_onehot = [col for col in cat_cols if col not in cols_labels]"
      ],
      "metadata": {
        "id": "-1QL-pENhz16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoding = df[feat_selction+['MIS_Status']].copy()\n",
        "le = LabelEncoder()\n",
        "\n",
        "for col in cols_labels:\n",
        "    df_encoding[col] = le.fit_transform(df_encoding[col])\n",
        "\n",
        "\n",
        "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "one_hot_encoded = one_hot_encoder.fit_transform(df_encoding[cols_onehot])\n",
        "\n",
        "one_hot_df = pd.DataFrame(one_hot_encoded, columns=one_hot_encoder.get_feature_names_out(cols_onehot))\n",
        "\n",
        "df_encoded_one = pd.concat([df_encoding, one_hot_df], axis=1)\n",
        "\n",
        "df_encoded = df_encoded_one.drop(cols_onehot, axis=1)\n",
        "df_encoded"
      ],
      "metadata": {
        "id": "MmqJnY6TdBbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded.to_pickle(\"data_after_feat_eng.pkl\")"
      ],
      "metadata": {
        "id": "ukoC8I-QGBvW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}